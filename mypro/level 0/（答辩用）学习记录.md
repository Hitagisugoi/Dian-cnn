**三、机器学习部分重要概念***

1. 数据集

    1. 什么是数据集?
由数据样本组成的集合。
样本之间是独立的（不依赖其他样本），单个样本拿出来仍然可以称为此目标的样本。（最好）没有必然联系（除目标外），比如飞机和蓝天，如果数据集中的飞机都出现在蓝天里，结果可能将蓝天也分类为飞机，或其他背景中的飞机不会被识别。
    
    2. 什么样的数据集是好的？样本数量足够多,分布比较广（尽可能包含目标物体所有情况）

    3. 怎样划分数据集？
我们获得数据后要对其进行划分，数据集一般包括：

        训练集（Training Set）：模型用于训练和调整模型参数。

        验证集（Validation Set）：用来验证模型精度和调整模型超参数，选择模型。

        测试集（Test Set）：测试模型的泛化能力，最终对模型评估。

        因为训练集和验证集是分开的，所以模型在验证集上面的精度在一定程度上可以反映模型的泛化能力。在划分验证集的时候，需要注意验证集的分布应该与测试集尽量保持一致，不然模型在验证集上的精度就失去了指导意义。
在使用数据集训练模型之前，我们需要先将整个数据集分为训练集、验证集、测试集。训练集是用来训练模型的，通过尝试不同的方法和思路使用训练集来训练不同的模型，再通过验证集使用交叉验证来挑选最优的模型，通过不断的迭代来改善模型在验证集上的性能，最后再通过测试集来评估模型的性能。如果数据集划分的好，可以提高模型的应用速度。如果划分的不好则会大大影响模型的应用的部署，甚至可能会使得我们之后所做的工作功亏一篑。

2. 损失函数(loss function)

衡量神经网络在训练数据上的性能，即网络是如何朝正确的方向前进。


3. 神经网络的结构（及各部分概念）

**预备知识**

1. 阶层型的神经网络主要结构

通过输入层激活信号，再通过隐藏层提取特征，不同隐藏层神经单元对应不同输入层的神经单元权重和自身偏置均可能不同，输入层兴奋传递到隐藏层兴奋，最后输出层根据不同的隐藏层权重和自身偏置输出结果。

2. 各个层如何向上传递信息

根据上边的介绍可知，输入层每个神经单元直接对应原始数据，然后向隐藏层提供信息，隐藏层每个神经单元对不同的输入层神经单元有不同的权重，从而偏向于对某种识别模式兴奋；多个隐藏层的神经单元兴奋后，输出层的神经单元根据不同隐藏层的兴奋加上权重后，给到不同的兴奋度，这个兴奋度就是模型最终识别的结果。

3. 神经网络中权重和偏置的作用

根据上述信息可知，权重会影响神经单元对输入信息敏感程度，比如隐藏层的神经单元通过控制权重形成识别模式偏向，输出层的神经单元调整对隐藏层神经单元的权重，可以形成输出结果的偏向；
而偏置，可以理解为敏感度，如果没有设置合适的偏置，一些“噪音”就会影响模型识别的结果，或者一些本该被识别出来的场景，但是在传递过程中被屏蔽掉了。

4. 有监督学习下，如何确认权重

在这里需要引入一个概念，『损失函数』又称为代价函数（cost function），计算方法为预测值与学习资料中偏差值之和(误差)的平方，有监督学习就是经过一些『学习资料』的训练，让模型预测的『误差』尽量的小。

5. 总结

神经网络由多个神经单元组成，深度学习中的神经网络可分为三层，分别是 输入层、隐藏层和输出层，每一层都有对应的神经网络与下一层连接。
输入层主要用于获取输入的信息，比如黑白照片的像素是黑色的还是白色的，大小主要取决于输入信息规模；
隐藏层主要进行『特征提取』，调整权重让隐藏层的神经单元对某种模式形成反应；
输出层用于对接隐藏层并输出模型结果，调整权重以对不同的隐藏层神经元刺激形成正确的反应，输出的兴奋度即为结果；

***实际上，隐藏层和输出层都进行处理操作“加权输入”和“通过激活函数 y=a(z) ”***

其中，偏置（阈值）可以排除一些“噪音”影响，监督学习下通过『损失函数』来衡量模型是否合理，计算方法是 预测值与正解之差(误差) 求和再平方；目标是通过学习资料的训练，让误差更小。

**激活函数**

1. 什么是激活函数

 神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。

2. 激活函数的用途（为什么需要激活函数）
   1. 对于y=ax+b 这样的函数，当x的输入很大时，y的输出也是无限大小的，经过多层网络叠加后，值更加膨胀的没边了，这显然不符合我们的预期，很多情况下我们希望的输出是一个概率       
   2. 线性的表达能力太有限了:  如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，即使经过多层网络的叠加，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。

激活函数的作用: 激活函数是用来加入非线性因素的，因为线性模型的表达能力不够，引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）


**卷积神经网络的结构**

- 卷积神经网络 (Convolutional Neural Network/CNN)

图像具有非常高的维数，因此训练一个标准的前馈网络来识别图像将需要成千上万的输入神经元，除了显而易见的高计算量，还可能导致许多与神经网络中的维数灾难相关的问题。 卷积神经网络提供了一个解决方案，利用卷积和池化层，来降低图像的维度。 由于卷积层是可训练的，但参数明显少于标准的隐藏层，它能够突出图像的重要部分，并向前传播每个重要部分。 传统的CNNs中，最后几层是隐藏层，用来处理“压缩的图像信息”。

![卷积神经网络 图解](https://pic2.zhimg.com/80/v2-61453795790a45f7b233b38828de3f7d_1440w.webp)

- softmax函数由Numpy模块和TensorFlow深度学习框架实现。与hardmax直接选取最大值的方式不同，softmax函数引入指数函数（***从而将每个输出值拉开距离***），将多分类的输出值转换为范围在0到1之间，和为1的概率分布。

- 加权输入z=wx+b，其中w为每个输入神经单元的值的权重，b(bias)为偏置，其相反数就是神经单元被激活的“阈值”。z可改写为两个向量的内积（通过添加一个虚拟输入值1），因为计算机擅长内积的计算。

- 交叉熵(crossentropyloss)是损失函数的一种。其计算方法的将从输出层输出的张量通过softmax

1. 优化器(optimizer)

基于训练数据和损失网络来更新网络的机制。

一言以蔽之，优化器就是在深度学习反向传播过程中，指引损失函数（目标函数）的各个参数往正确的方向更新合适的大小，使得更新后的各个参数让损失函数（目标函数）值不断逼近全局最小。（其实本质是梯度下降法）

- 随机梯度下降法（Stochastic Gradient Descent，SGD）

优点：

（1）每次只用一个样本更新模型参数，训练速度快

（2）随机梯度下降所带来的波动有利于优化的方向从当前的局部极小值点跳到另一个更好的局部极小值点，这样对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。

缺点：

（1）当遇到局部最优点或鞍点时，梯度为0，无法继续更新参数

（2）沿陡峭方向震荡，而沿平缓维度进展缓慢，难以迅速收敛

-  SGD with Momentum

为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。SGDM全称是SGD with momentum，在SGD基础上引入了一阶动量。

特点：

因为加入了动量因素，SGD-M缓解了SGD在局部最优点梯度为0，无法持续更新的问题和振荡幅度过大的问题，但是并没有完全解决，当局部沟壑比较深，动量加持用完了，依然会困在局部最优里来回振荡。

- Adam

Adam是前述方法的集大成者。SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。

优点：

（1）通过一阶动量和二阶动量，有效控制学习率步长和梯度方向，防止梯度的振荡和在鞍点的静止。

缺点：

（1）可能不收敛：

二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得Vt
可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。

（2）可能错过全局最优解：

自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。后期Adam的学习率太低，影响了有效的收敛。

2. 前向传播和反向传播

反向传播的过程：

    1. 利用前向传播求出误差E，
    2. 求出误差E对权重W的偏导数，
    3. 利用，权重更新公式（梯度下降法），更新权重W，其中  α  是学习率
    4. 继续反向传播，更新更接近输入层的权重W，直到更新所有的权重W，
    5. 循环1,2,3,4过程，不断更新权重W，降低误差E，最终得到训练好的神经网络（即适合的权重W）